\begin{problem}
  Given a set $N$ of points $\bp_i = (x_i, y_i), i \in \set{1, \ldots, N}$,
  in the image plane, we wish to find the best line passing through those points.

  \begin{enumroman}
    \item One way to solve this problem is to find $(a, b)$ that most closely
      satisfy the equations $y_i = ax_i + b$, in a least-squares sense.
      Write these equations in the form of a \emph{heterogeneous}
      least-squares problem $A \bx = \bb$, where $\bx = (a, b)^T$,
      and give an expression for the least-squares estimate of $\bx$.
      Give a geometric interpretation of the error being minimized,
      and use a simple graph to visualize the error.
      Does this make sense when fitting a line to points in an image?
      \begin{answer}
        The set of $N$ points satisfies the following system of equations:
        \begin{align*}
          y_1 &= ax_1 + b \\
          y_2 &= ax_2 + b \\
          \vdots \\
          y_N &= ax_N + b.
        \end{align*}
        % \newpage
        We can rewrite the system as the following heterogeneous
        \emph{least-squares} problem:

        \[
          \underbrace{\begin{bmatrix}
            x_1 & 1 \\
            x_2 & 1 \\
            \vdots & \vdots \\
            x_N & 1
          \end{bmatrix}}_{A}
          \underbrace{\begin{bmatrix}
            a \\
            b
          \end{bmatrix}}_{\bx}
          =
          \underbrace{\begin{bmatrix}
            y_1 \\
            y_2 \\
            \vdots \\
            y_N
          \end{bmatrix}}_{\bb}.
        \]

        We would like to multiply both sides by the inverse of the matrix,
        but the matrix is not square.
        To get around this, left-multiply both sides by $A^T$
        so that the matrix on the left becomes square,
        then solve the normal equations $A^T A \bx = A^T \bb$.

        \begin{align*}
          \underbrace{\begin{bmatrix}
            x_1 & x_2 & \cdots & x_N \\
            1 & 1 & \cdots & 1
          \end{bmatrix}}_{A^T}
          \underbrace{\begin{bmatrix}
            x_1 & 1 \\
            x_2 & 1 \\
            \vdots & \vdots \\
            x_N & 1
          \end{bmatrix}}_{A}
          \underbrace{\begin{bmatrix}
            a \\
            b
          \end{bmatrix}}_{\bx}
          &=
          \underbrace{\begin{bmatrix}
            x_1 & x_2 & \cdots & x_N \\
            1 & 1 & \cdots & 1
          \end{bmatrix}}_{A^T}
          \underbrace{\begin{bmatrix}
            y_1 \\
            y_2 \\
            \vdots \\
            y_N
          \end{bmatrix}}_{\bb} \\ \\
          \underbrace{\begin{bmatrix}
            \sum\limits_{i=1}^N x_i^2 & \sum\limits_{i=1}^N x_i \\
            \sum\limits_{i=1}^N x_i & N
          \end{bmatrix}}_{A^T A}
          \underbrace{\begin{bmatrix}
            a \\
            b
          \end{bmatrix}}_{\bx}
          &=
          \underbrace{\begin{bmatrix}
            \sum\limits_{i=1}^N x_i y_i \\
            \sum\limits_{i=1}^N y_i
          \end{bmatrix}}_{A^T \bb}
        \end{align*}

        Square matrix on the left is invertible,
        ~\footnote{
          If the matrix is not invertible, then the system of equations
          has either no solution (i.e. is inconsistent)
          or infinitely many solutions.
        }
        so we can solve for $\bx$ by left-multiplying both sides
        by its inverse:

        \begin{align*}
          \underbrace{\begin{bmatrix}
            a \\
            b
          \end{bmatrix}}_{\bx}
          &=
          \underbrace{\begin{bmatrix}
            \sum\limits_{i=1}^N x_i^2 & \sum\limits_{i=1}^N x_i \\
            \sum\limits_{i=1}^N x_i & N
          \end{bmatrix}^{-1}}_{(A^T A)^{-1}}
          \underbrace{\begin{bmatrix}
            \sum\limits_{i=1}^N x_i y_i \\
            \sum\limits_{i=1}^N y_i
          \end{bmatrix}}_{A^T \bb}
        \end{align*}

        % \newpage
        The error being minimized is the sum of the square
        \emph{vertical} distances of each point from the line,
        or equivalently, the sum of the square of how much
        each point deviates from the line.
        \[ \calE = \sum_{i=1}^N \abs{\abs{y_i - 
            \begin{bmatrix} a & b \end{bmatrix}
            \begin{bmatrix} x_i \\ 1 \end{bmatrix}}
          }^2. \]

        % draw tikz graph visualizing error
        \begin{figure}[H]
          \centering
          %  scale down to 0.6\textwidth
          \begin{tikzpicture}[domain=0:10]
            \draw[very thin,color=gray] (-1.1,-1.1) grid (9.9,9.9);

            % initialize total error
            \pgfmathsetmacro{\totalerror}{0}

            % text expression for terms in summation
            \pgfmathsetmacro{\sumexpression}{0}

            % add points and calculate total error
            \foreach \x/\y in {1/4, 2/1, 3/4, 4/3, 5/3, 6/1, 7/6, 8/6, 9/8}{

              %? draw point and offset from line
              % increase the size of the points (twice as big)
              \begin{scope}[every node/.style={scale=1.5}]
                \foreach \x/\y in {1/4, 2/1, 3/4, 4/3, 5/3, 6/1, 7/6, 8/6, 9/8}{
                  \node at (\x, \y) {\textbullet};
                }
              \end{scope}

              \pgfmathsetmacro{\error}{(\y-\x)*(\y-\x)}
              \draw[-, color=red] (\x, \x) -- (\x, \y) node[left] {$\green{\mathbf{\error}}$};
              
              %? concatenate '+ \error' to sum expression
              \xdef\sumexpression{\sumexpression + \error}
              
              %? increment total error
              \pgfmathparse{\totalerror + \error}
              \global\let\totalerror\pgfmathresult
            }

            % display total error
            \node at (5, -2) {$\calE = \sumexpression = \pgfmathprintnumber{\totalerror}$};

            \draw[->] (-0.2,0) -- (10.2,0) node[right] {$x$};
            \draw[->] (0,-1.2) -- (0,10.2) node[above] {$f(x)$};
          
            \draw[color=blue]    plot (\x,\x)   node[right] {$f(x) =x$};

          \end{tikzpicture}
          \caption{Graph of points, fitted line, and error measures.}
        \end{figure}

        The error associated with each point varies quadratically;
        twice the offset results in four times the error.
        This is often desirable when fitting a line to points in an image
        since it penalizes points that are farther from the line by a
        greater scale than points that are closer to the line.
        \emph{
          However, if there are outliers in the dataset,
          then using this method to fit a line to points in an image
          can result in one outlier point having a large effect on the
          fitted line and making it less accurate.
        } \\
      \end{answer}

    \newpage
    \item Another way to solve this problem is to find
      $\ell = (a, b, c)$, defined up to scale, that
      most closely satisfies the equations $ax_i + by_i + c = 0$,
      in a least-squares sense.
      Write these equations in the form of a \emph{homogeneous}
      least-squares problem $A \ell = 0$, where $\ell = (a, b, c)^T$
      and $\ell \ne 0$.
      This problem has a trivial solution (zero vector) which is not
      of much use. Describe some ways of avoiding this trivial solution
      and corresponding algorithms for solving the resulting
      optimization problem. Is this approach more or less
      useful than the previous one? Why?
      
      \step
      \emph{Hint: Think about how we can express the distance of a
      point from a line.}
      \begin{answer}
        The set of $N$ points satisfies the following system of equations:
        \begin{align*}
          ax_1 + by_1 + c &= 0 \\
          ax_2 + by_2 + c &= 0 \\
          \vdots \\
          ax_N + by_N + c &= 0.
        \end{align*}
        We can rewrite the system as the following homogeneous
        \emph{least-squares} problem:
        \[
          \underbrace{\begin{bmatrix}
            x_1 & y_1 & 1 \\
            x_2 & y_2 & 1 \\
            \vdots & \vdots & \vdots \\
            x_N & y_N & 1
          \end{bmatrix}}_{A}
          \underbrace{\begin{bmatrix}
            a \\
            b \\
            c
          \end{bmatrix}}_{\ell}
          =
          \underbrace{\begin{bmatrix}
            0 \\
            0 \\
            \vdots \\
            0
          \end{bmatrix}}_{\mathbf{0}}.
        \]

        % \newpage
        \step
        \textbf{On Avoiding the Trivial Solution} \\
        This equation has a trivial solution $\ell = \mathbf{0}$,
        which is not useful and can be ovoided by normalizing
        the vector $\ell$ so that $\Abs{\ell} = 1$.



        \step
        \textbf{On Finding the Solutions}
          We solve the equation $\bA\bx = 0$ by converting $\bA$ into
          its singular value decomposition
          \[
            A = U \Sigma V^T
          \]
          so that
          \[
            U \Sigma V^T \ell = 0.
          \]
          From the singular value decomposition, we compute $V$
          (the transpose of $V^T$) and find the column corresponding
          to the smallest singular value.
          This column contains the coefficients of the conic
          fitting the points.

        \step
        \textbf{On Comparative Utility of Approaches:}\\
        In the examples above, it is clear that using homogeneous least squares
        to fit a line to points in an image is much simpler to solve than
        using the heterogeneous approach,
        especially since in the homogeneous approach, the right-hand side
        of the equation is zero.
      \end{answer}
  \end{enumroman}
\end{problem}
