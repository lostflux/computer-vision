\section{Theory Questions}

\begin{problem}
  Assuming the affine warp model
  $\displaystyle \bW = \begin{bmatrix}
      1 + p_1 & p_3 & p_5 \\ p_2 & 1 + p_4 & p_6 \\ 0 & 0 & 1 \end{bmatrix}$,
  derive the expression for the Jacobian matrix
  $\bJ$ in terms of the warp parameters $\bp = [p_1, p_2, p_3, p_4, p_5, p_6]^\top$.

  \begin{answer}
    The Jacobian matrix $\bJ$ is the matrix of partial derivatives of the warp
    $\bW$ with respect to the warp parameters $\bp$. We have
    \begin{align*}
      \bW &= \begin{bmatrix}
        1 + p_1 & p_3 & p_5 \\ p_2 & 1 + p_4 & p_6 \\ 0 & 0 & 1
      \end{bmatrix} \\
      \bJ &= \begin{bmatrix}
        \frac{\partial \bW}{\partial p_1} & \frac{\partial \bW}{\partial p_2}
        & \frac{\partial \bW}{\partial p_3} & \frac{\partial \bW}{\partial p_4}
        & \frac{\partial \bW}{\partial p_5} & \frac{\partial \bW}{\partial p_6}
      \end{bmatrix}
    \end{align*}
    We can compute the partial derivatives of $\bW$ with respect to the warp
    parameters $\bp$ as follows:
    \begin{alignat*}{5}
      &\frac{\partial \bW}{\partial p_1} &= \begin{bmatrix}
        x \\ 0
      \end{bmatrix}, \qquad
      &&\frac{\partial \bW}{\partial p_2} &= \begin{bmatrix}
        0 \\ x
      \end{bmatrix}, \qquad
      &&\frac{\partial \bW}{\partial p_3} &= \begin{bmatrix}
      y \\ 0
      \end{bmatrix}, \\
      &\frac{\partial \bW}{\partial p_4} &= \begin{bmatrix}
        0 \\ y
      \end{bmatrix}, \qquad
      &&\frac{\partial \bW}{\partial p_5} &= \
      \begin{bmatrix}
        1 \\ 0
      \end{bmatrix}, \qquad
      &&\frac{\partial \bW}{\partial p_6} &= \begin{bmatrix}
        0 \\ 1
      \end{bmatrix}
    \end{alignat*}
    Therefore, the Jacobian matrix $\bJ$ is:
    \[
      \blue{\bJ = \begin{bmatrix}
        x & 0 & y & 0 & 1 & 0 \\ 0 & x & 0 & y & 0 & 1
      \end{bmatrix}}
    \]
    
    \emph{
      \underline{Note}: Since the last row of $\bW$ is always $[0, 0, 1]$
      and therefore leaves the $x$-coordinate (in homogeneous coordinates)
      unchanged, we do not need to include the partial derivatives of the
      last row of $\bW$ with respect to the warp parameters $\bp$
      since the last row would be all zeros.
    }
  \end{answer}
\end{problem}

\newpage
\begin{problem}
  Find the computational complexity (Big O notation) for the initialization
  step (pre-computing $\bJ$ and $\bH^{-1}$) and for each runtime iteration
  (Equation 13) of the Matthews-Baker method:
  \[
    \Delta \bp^\ast = \bH^{-1} \bJ^\top \brackets{\bI(\bW(\bx; \bp)) - \bT}
  \]
  Express your answers in terms of $n$, $m$, and $p$, where
  \begin{enumroman}
    \item $n$ is the number of pixels in the template $\bT$,
    \item $m$ is the number of pixels in an input image $\bI$, and
    \item $p$ is the number of parameters used to describe the warp $\bW$.
  \end{enumroman}
  How does this compare to the runtime of the regular Lucas-Kanade method?
  \begin{answer}
    \underline{Matthews-Baker method}:
    \begin{enumarabic}
      \item \textbf{Pre-computing $J$ and $H^{-1}$:}
        \begin{itemize}
          \item $J$ is the Jacobian of the image warp with respect to the parameters,
            which involves computing the gradient at each pixel in the template $T$ for each parameter.
            This gives a computational complexity of $O(np)$.
          % \item $H$ is the Hessian matrix, computed as $J^T J$, with a complexity of'
          %   $O(n p^2)$ since it is a $p \times p$ matrix computed for each of the $n$ pixels.
          \item The inversion of the Hessian matrix $H^{-1}$ has a complexity of
            $O(p^3)$ using standard matrix inversion methods such as \textsc{LU}-decomposition.
          \item \blue{
            In total, the computational complexity of the initialization step is
            $O(np + p^3)$.
          }
        \end{itemize}

      \item \textbf{Runtime Iteration (Equation 13):}
        \begin{itemize}
          \item Computing $[I(W(x; p)) - T]$ involves warping points from the input image
              (\blue{$O(np)$ since we only warp the template region of the input image})
              and then computing the difference from the template, which has a complexity of $O(n)$,
              for a total of $O(np)$.
          \item The computation of $J^T [I(W(x; p)) - T]$ involves multiplying a
            $p \times n$ matrix with an $n \times 1$ vector, which gives complexity of $O(n p)$.
          \item The matrix-vector multiplication $\blue{H^{-1}} \crim{J^T [I(W(x; p)) - T]}$
            has a computational complexity of $O(p^2)$ since it involves multiplying a
            $p \times p$ matrix with a $p \times 1$ vector.
          \item \blue{
            In total, this gives a computational complexity of $O(np + p^2)$.
            Given that $n \gg p > 1$, this complexity is dominated by $O(np)$.
          }
        \end{itemize}
    \end{enumarabic}

    \step
    \underline{Lucas-Kanade method}: \\
    In comparison, the regular Lucas-Kanade method has a computational complexity of
    $O(np^2 + p^3)$ per iteration. This is larger than the complexity of the Matthews-Baker method
    because the Matthews-Baker method is able to factor out the computation of the Jacobian and
    Hessian matrixes out of the iteration, so they are only computed once in the initialization step.
  \end{answer}
\end{problem}
